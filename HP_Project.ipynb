{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUP/rlGgDrE2afim6ei8OK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sundaynot/HP_Big_data_project/blob/main/HP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BIG DATA PROJECT**"
      ],
      "metadata": {
        "id": "PZPl4WFNeBBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to analyse the seven *Harry Potter* books, written by *J.K.Rowling* between 1997 and 2007."
      ],
      "metadata": {
        "id": "d-UINYdad4nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Initialize some libraries\n"
      ],
      "metadata": {
        "id": "cMgoDk26ftPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark graphframes\n",
        "print(\"Pyspark e Graphframes (Python) installati.\")"
      ],
      "metadata": {
        "id": "KWVYPe9Me5s8",
        "outputId": "ab728c21-2704-484e-ebf5-0b6eba483a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.12/dist-packages (0.6)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from graphframes) (2.0.2)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.12/dist-packages (from graphframes) (1.3.7)\n",
            "Pyspark e Graphframes (Python) installati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Create the SparkSession and Clone the repo from personal Github\n"
      ],
      "metadata": {
        "id": "1ydw2w7ogDH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"HP_Analysis\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.2-s_2.12\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Session created with GraphFrames package!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "A0T8f4cqisvG",
        "outputId": "0e7bdefd-5d2f-47fd-a9cd-efff571cfd53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkRuntimeError",
          "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1798103422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HP_Analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jars.packages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graphframes:graphframes:0.8.3-spark3.2-s_2.12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Session created with GraphFrames package!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 raise PySparkRuntimeError(\n\u001b[0m\u001b[1;32m    108\u001b[0m                     \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"JAVA_GATEWAY_EXITED\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder's name and repository's URL\n",
        "repo_name = \"HP_Big_data_project\"\n",
        "repo_url = \"https://github.com/sundaynot/HP_Big_data_project.git\"\n",
        "\n",
        "# If repo doesn't exist create, else print\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning repo '{repo_name}'...\")\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(f\"Repository '{repo_name}' existing.\")"
      ],
      "metadata": {
        "id": "V9wQoZgJjMc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful libraries\n",
        "from pyspark.sql import DataFrame, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (ArrayType, StructType, StructField, IntegerType, DoubleType)\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "iujKLBcejqiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Read .txt files with Spark"
      ],
      "metadata": {
        "id": "28kz53WLhLJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp1 = spark.read.text(\"/content/HP_Big_data_project/database/01 Harry Potter and the Sorcerers Stone.txt\")\n",
        "df_hp2 = spark.read.text(\"/content/HP_Big_data_project/database/02 Harry Potter and the Chamber of Secrets.txt\")\n",
        "df_hp3 = spark.read.text(\"/content/HP_Big_data_project/database/03 Harry Potter and the Prisoner of Azkaban.txt\")\n",
        "df_hp4 = spark.read.text(\"/content/HP_Big_data_project/database/04 Harry Potter and the Goblet of Fire.txt\")\n",
        "df_hp5 = spark.read.text(\"/content/HP_Big_data_project/database/05 Harry Potter and the Order of the Phoenix.txt\")\n",
        "df_hp6 = spark.read.text(\"/content/HP_Big_data_project/database/06 Harry Potter and the Half-Blood Prince.txt\")\n",
        "df_hp7 = spark.read.text(\"/content/HP_Big_data_project/database/07 Harry Potter and the Deathly Hallows.txt\")"
      ],
      "metadata": {
        "id": "dhYNjEaIj_r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3.1) Show the first row of each file (to see if there are errors)"
      ],
      "metadata": {
        "id": "5hMettzyhZQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp1.show(1, truncate=False)\n"
      ],
      "metadata": {
        "id": "xfMaW2BCoPuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp2.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "ojFhg_A_pSCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp3.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "kVhLCpzLpXu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp4.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "68jRuFOPpanf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp5.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "EabMam-2pcSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp6.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "Je36XXmlpd70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hp7.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "onFiprrfpgY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) Let's process the text"
      ],
      "metadata": {
        "id": "4akd9-6RigwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4.1) Define a function to find the chapters and their relatives names"
      ],
      "metadata": {
        "id": "uBHQH1pMkti0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_book_chapters(df_raw, book_number):\n",
        "    \"\"\"\n",
        "    Process a raw DataFrame of a book's text and segments it into chapters,\n",
        "    recognizing various heading formats (\"CHAPTER I\", \"Chapter 2 - Title\", etc.).\n",
        "    \"\"\"\n",
        "    # PHASE 1: Initially cleaning\n",
        "    # Add to the raw DataFrame a new column \"row_id\", and mantain the original order of the text (increasing id for each row)\n",
        "    # Filter removing empty rows and rows composed just by spaces\n",
        "    # Create a Window that mantains the original order of the text (like the row_id)\n",
        "\n",
        "    df_ordered = df_raw.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
        "    df_cleaned = df_ordered.filter((F.col(\"value\").isNotNull()) & (F.trim(F.col(\"value\")) != \"\"))\n",
        "    window_spec = Window.orderBy(\"row_id\")\n",
        "\n",
        "    # PHASE 2: Create the first chapter\n",
        "    # (for the first chapters there aren't \"CHAPTER ONE\" or similar, they start with the first row of the book and end when there is \"CHAPTER TWO\" )\n",
        "    # So I decide to take the first row for the chapter's name\n",
        "\n",
        "    first_line_title = df_cleaned.first()[\"value\"]\n",
        "\n",
        "    # PHASE 3: CHAPTERS' MARKERS\n",
        "    # A regular expression (RegEx) to find \"CHAPTER\" or \"Chapter\"\n",
        "    # the relative number and the relative chapter's name\n",
        "\n",
        "    chapter_regex = r\"^(?:CHAPTER|Chapter|CAPTER|CHATER)\\s+([A-Za-z0-9]+)(?:\\s*[-–—:]\\s*(.+))?$\"\n",
        "\n",
        "    # add the columns \"chapter_match\" and \"is_new_chapter_line\"\n",
        "    # in \"chapter_match\" if the row in value matches with chapter_regex\n",
        "    # insert all the matched string, else an empty string\n",
        "    # in \"is_new_chapter_line\" if \"chapter_match\"!= \"\"\n",
        "    # insert TRUE, else insert FALSE\n",
        "\n",
        "    df_with_markers = df_cleaned.withColumn(\"chapter_match\",\n",
        "        F.regexp_extract(F.col(\"value\"), chapter_regex, 0)\n",
        "        ).withColumn(\"is_new_chapter_line\", F.col(\"chapter_match\") != \"\")\n",
        "\n",
        "    # add the column \"chapter_title_raw\"\n",
        "    # if \"is_new_chapter_line\"= TRUE\n",
        "    # in \"chapter_title_raw\" insert the part of index 2 of the matched string (chapter's name)\n",
        "\n",
        "    df_with_markers = df_with_markers.withColumn(\"chapter_title_raw\",\n",
        "        F.when(F.col(\"is_new_chapter_line\"), F.regexp_extract(F.col(\"value\"), chapter_regex, 2)))\n",
        "\n",
        "    # PHASE 4: CHAPTER'S NAME PROPAGATION\n",
        "    # If the regex doesn't find the chapter's name, search it in the next row\n",
        "    # I use lead to take the next row after the specified window\n",
        "    # add a new column \"chapter_title_marker\" where:\n",
        "    # insert lead_value if \"is_new_chapter_line\"==TRUE and \"chapter_title_raw\"=\"\"\n",
        "    # else insert \"chapter_title_raw\"\n",
        "\n",
        "    lead_value = F.lead(\"value\").over(window_spec)\n",
        "    df_with_titles = df_with_markers.withColumn(\"chapter_title_marker\",\n",
        "        F.when((F.col(\"is_new_chapter_line\")) & (F.col(\"chapter_title_raw\") == \"\"),\n",
        "            lead_value).otherwise(F.col(\"chapter_title_raw\")))\n",
        "\n",
        "    # add the new column \"chapter_title_propagated\" where\n",
        "    # fills down the last non-null chapter_name across the window\n",
        "    # and stores it in a new column called chapter_title.\n",
        "\n",
        "    df_with_titles = df_with_titles.withColumn(\"chapter_title_propagated\",\n",
        "        F.last(\"chapter_title_marker\", ignorenulls=True).over(window_spec))\n",
        "\n",
        "    # PHASE 5: CHAPTER'S ID\n",
        "    # add a new column chapter_id with the defaul value = 1\n",
        "    # and everytime is_new_chapter_line=TRUE add 1, else add 0\n",
        "\n",
        "    df_with_ids = df_with_titles.withColumn(\"chapter_id\",\n",
        "        F.lit(1) + F.sum(F.when(F.col(\"is_new_chapter_line\"), 1).otherwise(0)).over(window_spec))\n",
        "\n",
        "    # add the final column \"chapter_title\", where\n",
        "    # if \"chapter_id\"=1 (first chapter) insert the first row for the chapter's name\n",
        "    # else copy from\n",
        "\n",
        "    df_with_final_titles = df_with_ids.withColumn(\"chapter_title\",\n",
        "        F.when(F.col(\"chapter_id\") == 1, F.lit(first_line_title)\n",
        "        ).otherwise(F.col(\"chapter_title_propagated\")))\n",
        "\n",
        "    # PHASE 6: Cleaning the text, removing from value CHAPTER and CHAPTER'S NAME\n",
        "    # add the new column if \"is_title_line\"\n",
        "    # where if \"is_new_chapter_line\"=TRUE insert TRUE in the next row\n",
        "    # else insert FALSE\n",
        "\n",
        "    df_with_meta_flags = df_with_final_titles.withColumn( \"is_title_line\",\n",
        "        F.lag(\"is_new_chapter_line\", 1, False).over(window_spec) )\n",
        "\n",
        "    # Filter the text\n",
        "    # mantain the rows where is_new_chapter_line=FALSE and \"is_title_line\"=FALSE\n",
        "    # \" ~ \" equal to NOT\n",
        "\n",
        "    df_final_text = df_with_meta_flags.filter(\n",
        "        (~F.col(\"is_new_chapter_line\")) & (~F.col(\"is_title_line\")) )\n",
        "\n",
        "    # PHASE 7: GROUPBY\n",
        "    # Group-by equal chapter_id and equal chapter_title,\n",
        "    # rename the column \"value\" with \"lines\"\n",
        "    # Create a new column chapter_text by joining all strings in lines with spaces\n",
        "    # Create a new column \"book_id\" with the value = book_number from the function intestation\n",
        "    # select only book_id,chapter_id, chapter_title, and chapter_text and orders rows by chapter_id.\n",
        "\n",
        "    df_chapters = (\n",
        "        df_final_text\n",
        "        .groupBy(\"chapter_id\", \"chapter_title\")\n",
        "        .agg(F.collect_list(\"value\").alias(\"lines\"))\n",
        "        .withColumn(\"chapter_text\", F.concat_ws(\" \", F.col(\"lines\")))\n",
        "        .withColumn(\"book_id\", F.lit(book_number))\n",
        "        .select(\"book_id\", \"chapter_id\", \"chapter_title\", \"chapter_text\")\n",
        "        .orderBy(\"chapter_id\"))\n",
        "\n",
        "    # If initially there is one letter, one o more spaces and text put all together (N early -> Nearly)\n",
        "\n",
        "    df_chapters = df_chapters.withColumn(\n",
        "    \"chapter_text\",\n",
        "    F.regexp_replace(\n",
        "        F.trim(F.col(\"chapter_text\")),\n",
        "        r\"^(\\w)\\s+(.*)$\",\n",
        "        \"$1$2\"))\n",
        "\n",
        "    return df_chapters\n"
      ],
      "metadata": {
        "id": "kdLDzHmIIkz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe list\n",
        "all_hp_dfs = [df_hp1, df_hp2, df_hp3, df_hp4, df_hp5, df_hp6, df_hp7]\n",
        "\n",
        "processed_books_list = []\n",
        "\n",
        "print(\"Starting the elaboration...\")\n",
        "\n",
        "# Use book number to take count of the number of each book\n",
        "for i, df_book_raw in enumerate(all_hp_dfs):\n",
        "    book_number = i + 1\n",
        "    print(f\"Processing book {book_number}...\")\n",
        "    df_processed = process_book_chapters(df_book_raw, book_number)\n",
        "    processed_books_list.append(df_processed)\n",
        "print(\"Work done.\")\n",
        "\n",
        "# Only one DataFrame\n",
        "all_chapters_df = reduce(DataFrame.unionAll, processed_books_list)\n",
        "all_chapters_df.cache()\n",
        "print(f\"Total chapters: {all_chapters_df.count()}\")\n",
        "\n",
        "# Show the result\n",
        "all_chapters_df.orderBy(\"book_id\", \"chapter_id\").show(18,truncate=20)"
      ],
      "metadata": {
        "id": "YnEzxdUZLsii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SQL TempView\n",
        "all_chapters_df.createOrReplaceTempView(\"harry_potter_saga\")\n",
        "\n",
        "# SQL Query to know how many chapters in each book\n",
        "spark.sql(\"\"\"\n",
        "    SELECT book_id, COUNT(chapter_id) as num_chapters\n",
        "    FROM harry_potter_saga\n",
        "    GROUP BY book_id\n",
        "    ORDER BY book_id\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "kQdJx35TxMkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4.2) An interesting count\n"
      ],
      "metadata": {
        "id": "mwszoELVw7Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover # Importa la classe\n",
        "\n",
        "# PHASE 1: TOKENIZE\n",
        "# Take the column \"chapter_text\", all in lower case, and spit everytime there is a space (\\s)\n",
        "# Rename this column as \"word\"\n",
        "# select just 3 column: \"book_id\",\"chapter_id\" and \"word\"\n",
        "\n",
        "df_words = all_chapters_df.select(\"book_id\",\"chapter_id\",\n",
        "    F.explode(F.split(F.lower(F.col(\"chapter_text\")), r\"\\s+\")).alias(\"word\"))\n",
        "\n",
        "# PHASE 2: NORMALIZE\n",
        "# Remove not alphanumeric from the text\n",
        "# Remove words long only 1 letter\n",
        "df_cleaned_words = df_words.withColumn(\"word\",\n",
        "    F.regexp_replace(F.col(\"word\"), r\"[^\\w]\", \"\")\n",
        ").filter(F.col(\"word\") != \"\").filter(F.length(F.col(\"word\")) >= 2)\n",
        "\n",
        "# Create an array for each chapter\n",
        "# composed by the words without simbols and rename it \"words_array\"\n",
        "df_word_arrays = df_cleaned_words.groupBy(\"book_id\", \"chapter_id\").agg(\n",
        "    F.collect_list(\"word\").alias(\"words_array\"))\n",
        "\n",
        "#PHASE 3: CUSTOMIZE STOPWORDSREMOVER\n",
        "# Load StopWordsRemover (language = english)\n",
        "stop_words_list = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        "\n",
        "# Add other words\n",
        "custom_stop_words = stop_words_list + [\n",
        "    # Principal characters (Name and Surname)\n",
        "    \"harry\", \"potter\",\n",
        "    \"ron\", \"weasley\",\n",
        "    \"hermione\", \"granger\",\n",
        "    \"dumbledore\", \"albus\",\n",
        "    \"hagrid\",\n",
        "    \"voldemort\", \"tom\", \"riddle\",\n",
        "    \"snape\", \"severus\",\n",
        "    \"malfoy\", \"draco\",\n",
        "\n",
        "    # Titles\n",
        "    \"professor\", \"mr\", \"mrs\", \"miss\", \"madam\", \"lord\",\"harrys\",\n",
        "\n",
        "    # Narrative  verbs\n",
        "    \"said\", \"asked\", \"looked\", \"thought\", \"knew\", \"know\",\"saw\",\"come\",\n",
        "    \"didnt\", \"dont\", \"wasnt\", \"isnt\", \"its\", \"hes\", \"shes\",\"got\",\"seemed\",\n",
        "    \"get\",\"go\", \"see\",\"looking\",\"think\",\"hed\", \"going\", \"look\",\"im\",\n",
        "\n",
        "    # Others\n",
        "     \"one\", \"well\", \"like\",\"around\",\"still\",\"something\",\"right\",\"long\",\"head\",\"us\",\n",
        "     \"though\",\"time\",\"eyes\",\"face\",\"voice\", \"head\", \"little\", \"yes\", \"first\", \"never\"\n",
        "]\n",
        "\n",
        "# Initialize the remover on \"words_array\" and call the output column \"filtered_words\"\n",
        "remover = StopWordsRemover(\n",
        "    inputCol=\"words_array\",\n",
        "    outputCol=\"filtered_words\")\n",
        "\n",
        "remover.setStopWords(custom_stop_words)\n",
        "\n",
        "# PHASE 4: APPLICATION\n",
        "df_filtered_arrays = remover.transform(df_word_arrays)\n",
        "\n",
        "# From the cleaning DataFrame select \"book_id\",\"chapter_id\",\n",
        "# and \"word\" (explosed version of \"filtered_words\")\n",
        "df_meaningful_words = df_filtered_arrays.select(\"book_id\",\"chapter_id\",\n",
        "    F.explode(F.col(\"filtered_words\")).alias(\"word\"))\n",
        "\n",
        "# PHASE 5: WORDS COUNT FOR BOOK\n",
        "df_word_counts_per_book = (\n",
        "    df_meaningful_words.groupBy(\"book_id\", \"word\").count())\n",
        "\n",
        "windowSpec = Window.partitionBy(\"book_id\").orderBy(F.col(\"count\").desc())\n",
        "\n",
        "# Add a column \"rank\" for each row\n",
        "df_ranked_words = df_word_counts_per_book.withColumn(\"rank\", F.row_number().over(windowSpec))\n",
        "\n",
        "# Select only the 5 most used words for each books\n",
        "df_top5_word_per_book = df_ranked_words.filter(F.col(\"rank\") <=5)\n",
        "\n",
        "# Show the final result\n",
        "print(\"The five most used words for each books:\")\n",
        "df_top5_word_per_book.select(\"book_id\", \"word\", \"count\").orderBy(\"book_id\").show(35, truncate=False)\n"
      ],
      "metadata": {
        "id": "NNjH49KxMD8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis perfectly maps the unique narrative focus of each book by identifying its key characters, locations, and themes.\n",
        "\n",
        "Book 1: The Dursleys (uncle, dudley, vernon).\n",
        "\n",
        "Book 2: The new characters (lockhart, dobby) and the setting (school).\n",
        "\n",
        "Book 3: The Marauders (lupin, black, sirius).\n",
        "\n",
        "Book 4: The Triwizard Tournament (moody, crouch, cedric).\n",
        "\n",
        "Book 5: The conflict (sirius, umbridge) and the key locations (door, room).\n",
        "\n",
        "Book 6: The key characters (slughorn, ginny), the location (room), and the mystery (hand).\n",
        "\n",
        "Book 7: The themes (wand, death, away) and the location (room)."
      ],
      "metadata": {
        "id": "VuQqawhVQ60u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also the word \"back\" is the central. It represents:\n",
        "\n",
        "- The Return of Voldemort: The entire plot is driven by Voldemort \"coming back\" to power.\n",
        "\n",
        "- The Return to School: The narrative structure of the first six books is built on \"going back\" to Hogwarts.\n",
        "\n",
        "- Looking Back (The Past): So much of the plot is discovered by \"looking back\" into memories (the Pensieve, Tom Riddle's diary).\n",
        "\n",
        "- The Physical Action: Characters are constantly \"going back\" to rescue someone, \"coming back\" from a fight, or being \"held back.\"\n",
        "\n",
        "It's the narrative glue that holds the whole series together."
      ],
      "metadata": {
        "id": "S5-kMXmsWgST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists of enchantments\n",
        "spell_list = [\n",
        "    \"lumos\", \"nox\", \"accio\", \"stupefy\", \"expelliarmus\",\n",
        "    \"riddikulus\", \"obliviate\", \"incendio\", \"protego\",\n",
        "    \"sectumsempra\", \"alohomora\", \"crucio\", \"imperio\",\n",
        "    \"confringo\", \"diffindo\" ]\n",
        "\n",
        "multi_word_spells = {\n",
        "    \"avada kedavra\": \"avada_kedavra\",\n",
        "    \"expecto patronum\": \"expecto_patronum\",\n",
        "    \"petrificus totalus\": \"petrificus_totalus\",\n",
        "    \"wingardium leviosa\": \"wingardium_leviosa\"\n",
        "}\n",
        "\n",
        "# Complete list\n",
        "all_spell_tokens = spell_list + list(multi_word_spells.values())\n",
        "\n",
        "# PREPROCESSING: all the text in lower case\n",
        "temp_df = all_chapters_df.withColumn(\"processed_text\", F.lower(F.col(\"chapter_text\")))\n",
        "\n",
        "# From \"avada kedavra\" to \"avada_kedavra\")\n",
        "# (spell = what I search (avada kedavra), token = how substitute it (avada_kedavra))\n",
        "for spell, token in multi_word_spells.items():\n",
        "    temp_df = temp_df.withColumn(\"processed_text\",\n",
        "        F.regexp_replace(F.col(\"processed_text\"), spell, token))\n",
        "\n",
        "# Explode the processed text\n",
        "df_words_adv = temp_df.select(\"book_id\",\n",
        "    F.explode(F.split(F.col(\"processed_text\"), r\"\\s+\")).alias(\"word\"))\n",
        "\n",
        "# Remove not alphanumeric from the text, (mantain _)\n",
        "df_cleaned_words_adv = df_words_adv.withColumn(\"word\",\n",
        "    F.regexp_replace(F.col(\"word\"), r\"[^\\w_]\", \"\"))\n",
        "\n",
        "# Filter for our enchantments list ( if the word is an enchantment mantain, else remove)\n",
        "df_spells_adv = df_cleaned_words_adv.filter(\n",
        "    F.col(\"word\").isin(all_spell_tokens))\n",
        "\n",
        "# Count\n",
        "df_total_spell_counts_adv = (\n",
        "    df_spells_adv.groupBy(\"word\")\n",
        "    .count()\n",
        "    .orderBy(F.col(\"count\").desc()))\n",
        "\n",
        "print(\"Total count of enchantments:\")\n",
        "df_total_spell_counts_adv.show(truncate=False)"
      ],
      "metadata": {
        "id": "HdeM7oRcc9Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the most interesting insights from this data:\n",
        "\n",
        "1. ***Hope and Utility Outrank Attack***\n",
        "The most telling detail is that the top two spells are not combat-focused:\n",
        " - expecto_patronum (36): This is the thematic spell of the series. It's not an attack, but a defense against despair (Dementors).\n",
        "\n",
        " - accio (33): This is the utility spell. Its high frequency shows the characters' growth. They aren't just in duels; they are actively solving problems, retrieving items, and using magic in practical ways.\n",
        "\n",
        "2. ***The Data Proves Harry's Signature Spell***\n",
        "The core combat spells are stupefy (26) uses and expelliarmus (25). They are practically tied. Stupefy is the standard, expelliarmus is famously Harry's personal, signature spell.\n",
        "\n",
        "3. ***The Threat of the Unforgivable Curses***\n",
        "The series gets incredibly dark, and the data shows it. The Unforgivable Curses are all high on the list:\n",
        "\n",
        "- avada_kedavra (19)\n",
        "\n",
        "- crucio (14)\n",
        "\n",
        "- imperio (8)\n",
        "\n",
        "4. ***Famous vs. Frequent***\n",
        "This is a great insight into storytelling. wingardium_leviosa is arguably the most famous spell from the franchise (\"It's Levi-O-sa, not Levio-SAH!\").\n",
        "And yet, it was only used 5 times.\n",
        "In contrast, a simple utility spell like lumos (22) is used constantly but is far less \"famous.\"\n",
        "\n",
        "5. ***\"Book-Specific\" Spells***\n",
        "You can clearly see which spells defined the plot of a specific book:\n",
        "\n",
        "- riddikulus (16): This is almost certainly all from Book 3 (Prisoner of Azkaban) and the Boggart lessons.\n",
        "\n",
        "- sectumsempra (9): This is the dark mystery at the heart of Book 6 (The Half-Blood Prince).\n"
      ],
      "metadata": {
        "id": "Y8bRW1jIUtKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(5). TF-IDF"
      ],
      "metadata": {
        "id": "UVptIST6d27D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Other useful librarie\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# 1. Configure CountVectorizer\n",
        "# Input = filtered_words output = raw_features\n",
        "# minDF=2.0 --> \"ignore words that don't appear at least in 2 chapters\"\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\",outputCol=\"raw_features\",vocabSize=10000,minDF=2.0)\n",
        "\n",
        "# 2. Configure IDF\n",
        "# Takes raw_features and calculate TF-IDF points\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "# 3. Make the Pipeline to do consequences steps.\n",
        "pipeline = Pipeline(stages=[cv, idf])\n",
        "\n",
        "# 4. Train the pipeline with the data\n",
        "print(\"Starting the pipeline training (CV + IDF)...\")\n",
        "pipeline_model = pipeline.fit(df_filtered_arrays)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# 5. Apply the model transformation\n",
        "tfidf_df = pipeline_model.transform(df_filtered_arrays)\n",
        "\n",
        "# print(\"DataFrame with TF-IDF:\")\n",
        "# Sparse Vector\n",
        "# tfidf_df.select(\"book_id\", \"chapter_id\", \"tfidf_features\").show(truncate=80)"
      ],
      "metadata": {
        "id": "wKVqTSD7-Yva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Extract the vocabulary from the model\n",
        "# Create a new DataFrame where word = word and index = the relative index.\n",
        "vocabulary = pipeline_model.stages[0].vocabulary\n",
        "vocab_df = spark.createDataFrame(enumerate(vocabulary), [\"index\", \"word\"])\n",
        "\n",
        "# 7. Define and Apply the UDF\n",
        "# From a sparse Vector (es. (10000, [5, 25], [0.1, 0.8]))\n",
        "# to an arry of readable pairs: [ (5, 0.1), (25, 0.8) ]\n",
        "def vector_to_array(v):\n",
        "    return list(zip([int(i) for i in v.indices], [float(f) for f in v.values]))\n",
        "\n",
        "to_array_udf = F.udf(vector_to_array,\n",
        "    ArrayType(StructType([StructField(\"index\", IntegerType()),StructField(\"score\", DoubleType())])))\n",
        "\n",
        "# Add a new column \"scores_array\" to insert the UDF function results\n",
        "df_with_scores = tfidf_df.withColumn(\"scores_array\", to_array_udf(F.col(\"tfidf_features\")))\n",
        "\n",
        "# Explode the new column and rename it \"score_struct\"\n",
        "df_exploded = df_with_scores.select(\"book_id\",\n",
        "    F.explode(F.col(\"scores_array\")).alias(\"score_struct\"))\n",
        "\n",
        "# 8. Join with vocabulary to translate indexes in words\n",
        "df_word_scores = df_exploded.join(\n",
        "    vocab_df,df_exploded.score_struct.index == vocab_df.index\n",
        ").select(\"book_id\", \"word\", \"score_struct.score\")\n",
        "\n",
        "# Group by to eliminate duplicates\n",
        "df_word_scores = df_word_scores.groupBy(\"book_id\", \"word\") \\\n",
        "    .agg(F.max(\"score\").alias(\"score\"))\n",
        "\n",
        "# 9. Find the 5 most important words for each book\n",
        "windowSpec = Window.partitionBy(\"book_id\").orderBy(F.col(\"score\").desc())\n",
        "\n",
        "df_top_words = df_word_scores.withColumn(\"rank\", F.row_number().over(windowSpec)) \\\n",
        "                            .filter(F.col(\"rank\") <= 5) \\\n",
        "                            .orderBy(\"book_id\", \"rank\")\n",
        "\n",
        "print(\"The five most important words for each book are (using TF-IDF):\")\n",
        "df_top_words.show(n=336, truncate=False)"
      ],
      "metadata": {
        "id": "29PZTsnu-bg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Harry Potter and the Philosopher's Stone**\n",
        "\n",
        "***quirrell***: The definition of a TF-IDF hit. He is the central villain for this book and never appears again.\n",
        "\n",
        "***dursley, vernon, dudley***: During the Book 1 a lot of text explain the Muggle world before Hogwarts compared to the sequels.\n",
        "\n",
        "***ronan***: The Centaur. Marks the first significant plot point in the Forbidden Forest.\n",
        "\n",
        "**2. Harry Potter and the Chamber of Secrets**\n",
        "\n",
        "***dobby***: The entire plot is driven by his attempts to \"save\" Harry.\n",
        "\n",
        "***lockhart***: The exclusive Defense Against the Dark Arts teacher for this specific year.\n",
        "\n",
        "***car, bludger***: Unique plot devices, the Flying Ford Anglia and the tampered Bludger are specific of this book.\n",
        "\n",
        "**3. Harry Potter and the Prisoner of Azkaban**\n",
        "\n",
        "***marge***: She appears in only one chapter.\n",
        "\n",
        "***stan, ern***: The Knight Bus. The algorithm picks these up because the dialogue on the bus is repetitive and condensed, creating a statistical spike for these characters who rarely appear elsewhere.\n",
        "\n",
        "***pettigrew***: The central mystery of the plot (Scabbers).\n",
        "\n",
        "***4. Harry Potter and the Goblet of Fire***\n",
        "\n",
        "***frank***: Frank Bryce is the protagonist of Chapter 1 and then disappears.\n",
        "\n",
        "***winky, crouch***: The House-Elf subplot.\n",
        "\n",
        "***cedric***: The tragic anchor of the Triwizard Tournament.\n",
        "\n",
        "**5. Harry Potter and the Order of the Phoenix**\n",
        "\n",
        "***umbridge***: She is present in almost every chapter as the antagonist, dominating the text frequency.\n",
        "\n",
        "***prophecy***: The entire plot revolves to retrieve this object.\n",
        "\n",
        "***ter***: A linguistic artifact. This is Hagrid's accent (phonetic \"to\"). It spikes here because Hagrid has massive monologues explaining his journey to the Giants, repeating this non-standard word hundreds of times.\n",
        "\n",
        "**6. Harry Potter and the Half-Blood Prince**\n",
        "\n",
        "***prime***: The political context. Refers to the Muggle Prime Minister.\n",
        "\n",
        "***ogden, morfin***: The Pensieve Memories. These are not present-day characters, but figures from the Gaunt family flashbacks.\n",
        "\n",
        "***slughorn***: The new professor and the holder of the key memory.\n",
        "\n",
        "**7. Harry Potter and the Deathly Hallows**\n",
        "\n",
        "***xenophilius***: Luna's dad, used to explain the Deathly Hallows symbol.\n",
        "\n",
        "***griphook, greyback***: The story shifts away from Hogwarts classes to the Gringott's banks.\n",
        "\n",
        "***kreacher***: Unlike in Book 5, Kreacher becomes a pivotal ally here (leading the trio to the locket)."
      ],
      "metadata": {
        "id": "gIarNhvbuNCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(6). LSH\n"
      ],
      "metadata": {
        "id": "8AYFdIgHyWAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Normalizer, BucketedRandomProjectionLSH\n",
        "\n",
        "# STEP 1: Normalize\n",
        "# Normalize 'tfidf_features'\n",
        "# This is important to avoid that the lenght influences the result\n",
        "normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"normalized_features\", p=2.0)\n",
        "df_normalized = normalizer.transform(tfidf_df)\n",
        "\n",
        "# STEP 2: LSH Configuration\n",
        "# input = normalized_features , output = hashes\n",
        "# BucketLength: bucket's widht\n",
        "# NumHashTables: how many times do you want to try the operation?\n",
        "brp = BucketedRandomProjectionLSH(\n",
        "    inputCol=\"normalized_features\", outputCol=\"hashes\",\n",
        "    bucketLength=2.0, numHashTables=3)\n",
        "\n",
        "# Train the LSH model on normalized data\n",
        "lsh_model = brp.fit(df_normalized)\n",
        "df_hashed = lsh_model.transform(df_normalized)\n",
        "\n",
        "print(\"Hashing complete. Seraching for similarities...\")\n",
        "\n",
        "# STEP 3: Find similar chapters\n",
        "# threshold=1.2: distance threshold, using the Euclidean Distance measure\n",
        "pairs = lsh_model.approxSimilarityJoin(df_hashed, df_hashed, threshold=1.2, distCol=\"EuclideanDistance\")\n",
        "\n",
        "# STEP 4: Cleaning\n",
        "# 1. Remove comparison between the same chapter of the same book (distance = 0)\n",
        "# 2. Remove duplicates ( book 1 chapt 2 and book 3 chapt 4 == book 3 chapt 4 and book 1 chapt 2)\n",
        "\n",
        "clean_pairs = pairs.filter(\n",
        "    (F.col(\"datasetA.book_id\") < F.col(\"datasetB.book_id\")) |\n",
        "    ((F.col(\"datasetA.book_id\") == F.col(\"datasetB.book_id\")) &\n",
        "     (F.col(\"datasetA.chapter_id\") < F.col(\"datasetB.chapter_id\")))\n",
        ").select(\n",
        "    F.col(\"datasetA.book_id\").alias(\"Book_A\"),\n",
        "    F.col(\"datasetA.chapter_id\").alias(\"Chapter_A\"),\n",
        "    F.col(\"datasetB.book_id\").alias(\"Book_B\"),\n",
        "    F.col(\"datasetB.chapter_id\").alias(\"Chapter_B\"),\n",
        "    F.format_number(F.col(\"EuclideanDistance\"), 4).alias(\"Distance\")\n",
        ").orderBy(\"Distance\")\n",
        "\n",
        "print(\"The most similar pairs of chapters are (Low distance = High Similarity):\")\n",
        "clean_pairs.show(30, truncate=False)"
      ],
      "metadata": {
        "id": "pip5ey4IyNHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. The Dursley Cluster***\n",
        "\n",
        "Chapters: 1-3 (The Letters from No One), 5-2 (A Peck of Owls), 4-4 (Back to the Burrow – though it starts at the Dursleys'), 2-1 (The Worst Birthday), 7-3 (The Departure of the Dursleys).\n",
        "\n",
        "Why: In these chapters, there is often an invasion of letters or owls at the Dursley household.\n",
        "\n",
        "Relevant words: Uncle, Vernon, Aunt, Petunia, Dudley, Letter, Owl, Kitchen, Scream, television, drill, living room.\n",
        "\n",
        "***2. Narrative Continuity***\n",
        "\n",
        "7-20 vs 7-21 (Xenophilius Lovegood & The Tale of the Three Brothers).\n",
        "\n",
        "Relevant words: Hallows, Wand, Peverell, Cloak, Stone.\n",
        "\n",
        "7-24 vs 7-25 (The Wandmaker & Shell Cottage).\n",
        "\n",
        "Relevant words: Griphook, Ollivander, Wand.\n",
        "\n",
        "Why: In the 7th book, the plot is a continuous stream (the journey in the tent), lacking the distinct school-year structure.\n",
        "\n",
        "***3. Grimmauld Place***\n",
        "\n",
        "Chapters: 5-6 (The Noble and Most Ancient House of Black) and 7-10 (Kreacher’s Tale).\n",
        "\n",
        "Why: Both chapters take place entirely inside Number 12, Grimmauld Place.\n",
        "\n",
        "Relevant words: Kreacher, Portrait, Sirius, Mother, Walburga, Regulus, Locket, Clean.\n",
        "\n",
        "***4. The Dobby Connection***\n",
        "\n",
        "Chapters: 2-2 (Dobby’s Warning) and 4-21 (The House-Elf Liberation Front).\n",
        "\n",
        "Why: in both chapters Dobby speaks obsessively to Harry.\n",
        "\n",
        "Relevant words: Dobby, Elf, Sir.\n",
        "\n",
        "***Conclusion***\n",
        "\n",
        "J.K. Rowling has lexical \"templates\".\n",
        "\n",
        "When she writes about the Dursleys, she always uses the same specific set of words (anger, Muggle objects), making those chapters mathematically isolated from the magical world.\n",
        "\n",
        "When the plot becomes static (Harry hiding or traveling in Book 7), adjacent chapters resemble each other closely because the setting does not change.\n"
      ],
      "metadata": {
        "id": "mmtGsrj8DzcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definiamo i personaggi principali (in minuscolo!)\n",
        "characters_list = [\n",
        "    \"harry\", \"ron\", \"hermione\", \"dumbledore\", \"voldemort\", \"snape\",\n",
        "    \"draco\", \"hagrid\", \"neville\", \"ginny\", \"lupin\", \"sirius\",\n",
        "    \"mcgonagall\", \"dobby\", \"kreacher\"]\n",
        "\n",
        "# 2. Creiamo il DataFrame dei Vertici (Vertices)\n",
        "# La colonna deve chiamarsi \"id\" per GraphFrames\n",
        "vertices = spark.createDataFrame([(c,) for c in characters_list], [\"id\"])\n",
        "\n",
        "print(\"Vertici (Nodi) del nostro grafo:\")\n",
        "vertices.show()"
      ],
      "metadata": {
        "id": "5SHvu43bTi3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumo che tu abbia il DataFrame 'df_filtered_arrays' con la colonna 'filtered_words'\n",
        "# o 'df_clean' con 'words'. Userò 'df_clean' con 'words' come esempio.\n",
        "# Assicurati che 'words' sia il nome della colonna con l'array di parole pulite.\n",
        "\n",
        "# 1. Creiamo un \"key\" univoco per capitolo e \"esplodiamo\" le parole\n",
        "df_words_per_chapter = df_filtered_arrays.select(\n",
        "    F.concat(F.col(\"book_id\"), F.lit(\"_\"), F.col(\"chapter_id\")).alias(\"chapter_key\"),\n",
        "    F.explode(F.col(\"filtered_words\")).alias(\"word\"))\n",
        "\n",
        "# 2. Filtriamo solo le parole che sono personaggi e rimuoviamo duplicati\n",
        "# (Ci interessa solo se sono menzionati, non quante volte)\n",
        "df_mentions = df_words_per_chapter.filter(\n",
        "    F.col(\"word\").isin(characters_list)).distinct()\n",
        "\n",
        "# 3. Il Self-Join (La magia è qui)\n",
        "df_A = df_mentions.alias(\"A\")\n",
        "df_B = df_mentions.alias(\"B\")\n",
        "\n",
        "print(\"Calcolo delle co-occorrenze (archi)...\")\n",
        "\n",
        "# Uniamo sullo stesso capitolo, ma solo dove il personaggio A è \"minore\" di B\n",
        "# Questo evita (Ron, Harry) se abbiamo già (Harry, Ron) e evita (Harry, Harry)\n",
        "df_edges_raw = df_A.join(df_B, on=\"chapter_key\") \\\n",
        "    .where(F.col(\"A.word\") < F.col(\"B.word\")) \\\n",
        "    .select(F.col(\"A.word\").alias(\"src\"), F.col(\"B.word\").alias(\"dst\"))\n",
        "\n",
        "# 4. Aggreghiamo gli archi: contiamo quante volte due personaggi sono apparsi insieme\n",
        "# 'weight' (peso) = il numero di capitoli in cui sono apparsi insieme\n",
        "edges = df_edges_raw.groupBy(\"src\", \"dst\").count().withColumnRenamed(\"count\", \"weight\")\n",
        "\n",
        "print(\"Archi (Edges) creati:\")\n",
        "edges.orderBy(F.col(\"weight\").desc()).show()"
      ],
      "metadata": {
        "id": "jZ2gzkTJTssL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes import GraphFrame\n",
        "\n",
        "# 1. Creiamo il grafo\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# 2. Eseguiamo PageRank\n",
        "# resetProbability = 0.15 (standard)\n",
        "# maxIter = 10 (numero di volte che l'algoritmo \"cammina\" nel grafo)\n",
        "print(\"Esecuzione di PageRank in corso...\")\n",
        "results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
        "\n",
        "# 3. Visualizziamo i risultati\n",
        "# Il PageRank è assegnato ai vertici (nodi)\n",
        "print(\"Classifica dei personaggi per centralità (PageRank):\")\n",
        "results.vertices.select(\"id\", \"pagerank\") \\\n",
        "    .orderBy(F.col(\"pagerank\").desc()) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "id": "juO2vkN6WhbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cOrfiSAl1Lrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Progetto 3: Costruire il Grafo Sociale (PageRank) 🧑‍🤝‍🧑\n",
        "Come suggerito, puoi vedere chi sono i personaggi \"centrali\".\n",
        "\n",
        "Crea una lista di personaggi principali: characters = [\"Harry\", \"Ron\", \"Hermione\", \"Dumbledore\", \"Voldemort\", \"Snape\", \"Draco\", ...]\n",
        "\n",
        "Crea gli \"archi\": Usa all_chapters_df. Per ogni capitolo, se due personaggi (es. \"Harry\" e \"Dumbledore\") sono menzionati entrambi nel chapter_text, crea un arco tra loro.\n",
        "\n",
        "Applica PageRank: Una volta creato il DataFrame degli archi (es. src, dst), puoi usare l'algoritmo PageRank (disponibile in Spark GraphFrames) per trovare il personaggio più importante."
      ],
      "metadata": {
        "id": "a9NaVe0n1RX_"
      }
    }
  ]
}